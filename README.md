# 🎸 ShamaOllama

A modern, professiona## 📚 Project Documentation

Explore the complete vision and technical details:

- **🗺️ [ROADMAP.md](ROADMAP.md)** - Development roadmap and future plans
- **🌟 [VISION.md](VISION.md)** - Project philosophy and long-term vision
- **🤝 [COMMUNITY.md](COMMUNITY.md)** - Contribution guide and collaboration opportunities
- **🏗️ [ARCHITECTURE.md](ARCHITECTURE.md)** - Technical architecture and design decisions
- **📋 [CONTRIBUTING.md](CONTRIBUTING.md)** - How to contribute to the project
- **🔒 [SECURITY.md](SECURITY.md)** - Security considerations and reporting

## 🎯 Why I Built ShamaOllama

### The Problem I Faced

After spending time exploring local AI with Ollama, I encountered a frustrating barrier: **there was no simple, user-friendly way to interact with these powerful models**.

While Ollama itself is incredible technology, the reality is that most people don't want to:

- Open terminal windows and type cryptic commands
- Remember complex model names and parameterserface for Ollama built with CustomTkinter. This application provides a clean, fast, and robust way to interact with your local Ollama installation with real-time streaming responses.

_Paying homage to "Shama Lama Ding Dong" from the classic comedy Animal House (1978)_

**Author:** John Blancuzzi  
**License:** MIT License  
**Version:** 1.0.0

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![CustomTkinter](https://img.shields.io/badge/GUI-CustomTkinter-green.svg)](https://github.com/TomSchimansky/CustomTkinter)
[![GitHub release](https://img.shields.io/github/release/BlancuzziJ/Ollama-GUI.svg)](https://github.com/BlancuzziJ/Ollama-GUI/releases)
[![GitHub issues](https://img.shields.io/github/issues/BlancuzziJ/Ollama-GUI.svg)](https://github.com/BlancuzziJ/Ollama-GUI/issues)
[![GitHub stars](https://img.shields.io/github/stars/BlancuzziJ/Ollama-GUI.svg)](https://github.com/BlancuzziJ/Ollama-GUI/stargazers)

![ShamaOllama Demo](https://via.placeholder.com/800x500/2b2b2b/ffffff?text=ShamaOllama+Demo)

> **⭐ If you find this project useful, please give it a star on GitHub!**

## 💖 Support the Project

If ShamaOllama brings value to your AI workflow, consider supporting its development:

<iframe src="https://github.com/sponsors/BlancuzziJ/card" title="Sponsor BlancuzziJ" height="225" width="600" style="border: 0;"></iframe>

**Why Support?**

- 🚀 **Faster Development** - More time for features and improvements
- 🐛 **Better Support** - Dedicated time for bug fixes and user assistance
- 🎯 **New Features** - Priority development of community-requested features
- 🌟 **Recognition** - Support open-source development and innovation

**Other Ways to Support:**

- ⭐ Star the repository on GitHub
- 🐛 Report bugs and suggest improvements
- 📢 Share ShamaOllama with others
- 💬 Join discussions and help other users

## � Why I Built ShamaOllama

### The Problem I Faced

After spending time exploring local AI with Ollama, I encountered a frustrating barrier: **there was no simple, user-friendly way to interact with these powerful models**.

While Ollama itself is incredible technology, the reality is that most people don't want to:

- Open terminal windows and type cryptic commands
- Remember complex model names and parameters
- Deal with command-line interfaces for everyday AI interaction
- Be forced to become "techy" just to use local AI

### The Vision

I wanted to create something that would:

🎯 **Bridge the Gap** - Make local AI accessible to everyone, not just developers  
🚀 **Provide a Starting Point** - Give the developer community an attractive foundation to build upon  
💡 **Solve Real Problems** - Create tools that end-users would actually want to use  
🌟 **Foster Innovation** - Inspire others to improve and expand the concept

### The Journey

This project represents my exploration into what local AI interaction _should_ feel like. Yes, it's in its infancy. Yes, Python with its dependencies might not be the ultimate solution. But that's exactly the point!

**ShamaOllama is designed to be:**

- A **proof of concept** that shows what's possible
- A **starting foundation** for the community to improve upon
- An **invitation** for developers to take this concept further

### The Future Possibilities

I'm excited to see where the community takes this. Will someone:

- Port it to JavaScript for web-based deployment?
- Rebuild it in C# for native Windows integration?
- Create mobile versions for iOS and Android?
- Add advanced features I haven't even imagined?

**That's the beauty of open source!** 🌟

### My Hope

If you're reading this, you probably understand the potential of local AI. My hope is that ShamaOllama helps bridge the gap between powerful technology and practical usability. Whether you use it as-is, fork it, or completely reimagine it in another language - **that's success to me**.

The goal isn't to create the perfect, final solution. It's to **prove that better user experiences are possible** and **inspire others to make them even better**.

---

## �🎵 About the Name

**ShamaOllama** pays tribute to the memorable song "Shama Lama Ding Dong" featured in the beloved 1978 comedy film _Animal House_. Just like that classic brought joy and laughter to audiences, ShamaOllama aims to bring a fun, engaging, and professional experience to AI interaction.

### The Deeper Meaning of "Shama"

The word "Shama" carries beautiful meanings across cultures that perfectly align with our vision:

- **Sanskrit**: "Equal," "even," "equanimity," "calmness," and "peace of mind" - representing the serene, balanced interaction with AI
- **Hebrew**: "To hear," "to listen," and "to obey" - embodying attentive AI conversation and responsive intelligence
- **Persian**: Derived from "candle" - symbolizing enlightenment and the light of knowledge that AI brings

These meanings reflect our goal: creating a peaceful, enlightening, and responsive AI interface that truly listens and brings clarity to your interactions.

## Table of Contents

- [💖 Support the Project](#-support-the-project)
- [🎵 About the Name](#-about-the-name)
- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [Screenshots](#screenshots)
- [Architecture](#architecture)
- [Contributing](#contributing)
- [License](#license)
- [Support](#support)

## 🚀 Quick Start

**⚡ Super Quick Start:** See [QUICKSTART.md](QUICKSTART.md) for the fastest way to get running!

### Prerequisites

- **[Ollama](https://ollama.ai/)** installed and running
- **Python 3.8+**
- **Git** (for cloning)

### 1-Minute Setup

#### Windows (The Easy Way)

```cmd
git clone https://github.com/BlancuzziJ/Ollama-GUI.git
cd Ollama-GUI
setup.bat      # Installs everything + offers desktop shortcut
run_gui.bat    # Starts ShamaOllama 🎉
```

#### Linux/Mac (The Easy Way)

```bash
git clone https://github.com/BlancuzziJ/Ollama-GUI.git
cd Ollama-GUI
./run_gui.sh   # Handles setup automatically 🎉
./create_desktop_shortcut_universal.sh  # Optional: Desktop shortcut
```

#### Manual Setup (All Platforms)

```bash
# Clone the repository
git clone https://github.com/BlancuzziJ/Ollama-GUI.git
cd Ollama-GUI

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Optional: Enhanced GPU detection
pip install -r requirements-gpu.txt

# Run the application
python main.py
```

### First Steps

1. **Start Ollama**: `ollama serve`
2. **Pull a model**: `ollama pull llama2` or use the GUI
3. **Start chatting**: Select model and begin conversation
4. **Check System Info**: Click "🖥️ System" tab for hardware analysis

## Screenshots

## Features

### 🎨 Modern Interface

- **Clean, professional design** with CustomTkinter
- **Dark/Light theme support** with one-click toggle
- **Responsive layout** that adapts to window resizing
- **Intuitive navigation** with sidebar panel switching

### 💬 Chat Interface

- **Real-time streaming responses** with live text generation
- **Animated typing indicators** so you know the AI is thinking
- **Visual progress tracking** with word/character count during streaming
- **Automatic fallback** to non-streaming mode if needed
- **Conversation history** with timestamps
- **Model switching** on the fly
- **Keyboard shortcuts** (Ctrl+Enter to send)
- **Professional message formatting** with role-based styling
- **🧠 Thinking models support** - Hide reasoning process for cleaner responses (DeepSeek, etc.)

### 🔧 Model Management

- **View all installed models** with size and modification info
- **Pull new models** with real-time progress tracking
- **Delete models** with batch operations and safety confirmations
- **Automatic model discovery** and refresh

### 🌐 Connectivity & Deployment

- **🏠 Local Ollama** - Works seamlessly with localhost installations
- **☁️ Remote Ollama** - Connect to any Ollama instance on your network or cloud
- **🐳 Docker Ready** - Full support for containerized deployments
- **🔒 Secure Connections** - HTTP and HTTPS support with certificate validation
- **👥 Team Collaboration** - Multiple users connecting to shared Ollama servers

### 📜 Chat History

- **Persistent chat sessions** saved automatically
- **Browse previous conversations** with search
- **Export conversations** to text or JSON format
- **Session management** with custom titles

### ⚙️ Settings & Configuration

- **🌐 Remote Ollama Support** - Connect to any Ollama instance (local or remote)
- **🔗 Flexible Connectivity** - HTTP/HTTPS support for cloud and team deployments
- **🧪 Connection testing** to verify Ollama availability
- **💾 Auto-save settings** for chat sessions
- **📊 Customizable history limits**

## Installation & Setup

### Prerequisites

- Python 3.8 or higher
- Ollama installed and running locally
- Virtual environment already set up (.venv)

### Quick Start

1. **Install dependencies:**

   ```bash
   .venv\Scripts\activate
   pip install -r requirements.txt
   ```

2. **Optional: Enhanced GPU Detection**

   For detailed hardware analysis and AI model recommendations:

   ```bash
   pip install -r requirements-gpu.txt
   ```

   Or use the convenience scripts:

   - Windows: `install-gpu-support.bat`
   - PowerShell: `install-gpu-support.ps1`
   - Linux/macOS: `install-gpu-support.sh`

   > **Note:** GPU detection works without these dependencies but provides basic information only. Enhanced detection includes detailed VRAM usage, system specifications, and personalized AI model recommendations.
   >
   > **Architecture:** ShamaOllama uses a plugin-like architecture where optional features degrade gracefully. The core application remains lightweight while enhanced features are available when needed. See `PLUGIN_ARCHITECTURE.md` for details.

3. **Run the application:**

   ```bash
   python main.py
   ```

   Or use the convenient batch file:

   ```bash
   run_gui.bat
   ```

4. **First-time setup:**
   - The app will automatically connect to `http://localhost:11434`
   - If Ollama is running elsewhere, update the URL in Settings
   - Pull your first model using the Models panel

## Usage Guide

### Getting Started

1. **Start Ollama** on your system
2. **Launch the GUI** using `run_gui.bat` or `python main.py`
3. **Pull a model** if you don't have any (try `llama2` or `mistral`)
4. **Select a model** from the dropdown in the sidebar
5. **Start chatting!**

### Navigation

- **💬 Chat**: Main conversation interface
- **🔧 Models**: Manage your Ollama models
- **📜 History**: Browse and export past conversations
- **⚙️ Settings**: Configure the application

### Keyboard Shortcuts

- `Ctrl + Enter`: Send message
- `Alt + N`: New chat session
- `F5`: Refresh models

### Model Management

Popular models to try:

- `llama2` - Meta's large language model
- `mistral` - Fast and capable model
- `codellama` - Specialized for coding tasks
- `llama2:13b` - Larger variant for better quality

### 🧠 Working with Thinking Models

Some advanced models like **DeepSeek** show their reasoning process before giving answers. While this can be educational, it can also make responses verbose and harder to read.

**ShamaOllama includes a special feature to handle thinking models:**

1. Go to **Settings** ⚙️
2. Enable **"Hide thinking/reasoning process"**
3. Chat with thinking models like DeepSeek
4. Enjoy cleaner, more concise responses!

This feature automatically filters out common thinking patterns like:

- `<thinking>...</thinking>` blocks
- `[thinking]...[/thinking]` sections
- `**Thinking:**` headers

**Note:** This only affects models that explicitly show thinking processes. Regular models work normally.

## Architecture

The application is built with a modular, extensible design:

```
OllamaGUI/
├── main.py           # Main application and GUI
├── OllamaAPI         # API communication layer
├── ChatManager       # Chat session and history management
├── requirements.txt  # Python dependencies
├── run_gui.bat       # Windows launcher script
└── README.md         # This file
```

### Key Components

- **OllamaAPI**: Handles all communication with Ollama
- **ChatManager**: Manages chat sessions, history, and persistence
- **OllamaGUI**: Main application with CustomTkinter interface

## Configuration

### Remote Ollama Support

**ShamaOllama supports connecting to remote Ollama instances!** This enables team collaboration, cloud deployments, and flexible infrastructure setups.

#### Connecting to Remote Ollama

1. **Open Settings Panel** in the application
2. **Update Ollama URL** to point to your remote instance:
   - `http://192.168.1.100:11434` (Local network)
   - `https://ollama.yourcompany.com` (Cloud/HTTPS)
   - `http://ollama-server:11434` (Docker containers)
3. **Test Connection** to verify connectivity
4. **Save Settings** to persist the configuration

#### Security Considerations for Remote Connections

- **Use HTTPS** for connections over the internet
- **Secure your network** - Ollama doesn't have built-in authentication
- **Consider VPN** for remote access to internal Ollama servers
- **Validate certificates** when using HTTPS connections

#### Common Remote Setup Scenarios

**Docker Compose Setup:**

```yaml
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
  shamollama:
    image: shamollama
    environment:
      - OLLAMA_URL=http://ollama:11434
```

**Team Server Setup:**

- Central Ollama server on powerful hardware
- Multiple team members connecting via ShamaOllama
- Shared model library and consistent performance

**Cloud Deployment:**

- Ollama running on cloud instances (AWS, GCP, Azure)
- ShamaOllama connecting via HTTPS with proper SSL
- Scalable infrastructure for enterprise use

### Settings File

Settings are automatically saved to `~/.shamollama/settings.json`:

```json
{
  "ollama_url": "http://localhost:11434",
  "auto_save": true,
  "max_history": "100"
}
```

**Key Settings:**

- `ollama_url`: The endpoint for your Ollama instance (local or remote)
- `auto_save`: Automatically save chat sessions
- `max_history`: Maximum number of chat sessions to retain

### Chat History

Chat history is stored in `~/.shamollama/chat_history.json` and automatically managed.

## Troubleshooting

### Common Issues

**"Cannot connect to Ollama"**

- **Local Connection**: Ensure Ollama is running: `ollama serve`
- **Remote Connection**: Verify the remote Ollama server is accessible
- **Network Issues**: Check firewall settings and network connectivity
- **URL Configuration**: Ensure the URL in Settings matches your Ollama installation
- **HTTPS Issues**: For HTTPS connections, verify SSL certificates are valid

**"No models available"**

- Pull a model first: Use the Models panel or run `ollama pull llama2`
- Refresh the models list using the Refresh button
- For remote Ollama: Ensure models are installed on the remote server

**Remote Connection Issues**

- **Timeout errors**: Check network latency and increase timeout settings
- **Certificate errors**: For HTTPS, ensure valid SSL certificates
- **Port access**: Verify port 11434 (or custom port) is accessible
- **Docker networking**: Ensure containers can communicate properly

**Application won't start**

- Activate virtual environment: `.venv\Scripts\activate`
- Install dependencies: `pip install -r requirements.txt`
- Check Python version: `python --version` (3.8+ required)

### Performance Tips

- Use smaller models (7B) for faster responses
- Close unnecessary applications for better performance
- Consider model-specific settings for optimal results

## Future Enhancements

Planned features for future updates:

- **Streaming responses** with real-time text display
- **Custom model parameters** (temperature, top_p, etc.)
- **Multi-model conversations** and comparison
- **Plugin system** for extensions
- **Advanced search** in chat history
- **Conversation templates** and prompts
- **Model performance metrics**
- **Export to multiple formats**

## 🛠️ Technical Choices & Future Vision

### Why Python + CustomTkinter?

**Current Stack:**

- **Python**: Rapid prototyping, excellent AI/ML ecosystem, cross-platform
- **CustomTkinter**: Modern, beautiful UI components with dark/light themes
- **Requests**: Simple, reliable HTTP client for Ollama API
- **Threading**: Responsive UI with background operations

**The Reality:** This is a **starting point**, not the end destination. Python with dependencies might not be the ideal final solution, but it's perfect for:

✅ **Rapid Development** - Get a working prototype quickly  
✅ **Easy Contribution** - Lower barrier for community involvement  
✅ **Cross-Platform** - Runs on Windows, Mac, and Linux  
✅ **Rich Ecosystem** - Leverage Python's AI/ML libraries

### 🚀 Community Opportunities

**This project is designed to inspire ports and improvements:**

#### 🌐 **Web-Based (JavaScript/TypeScript)**

- **Electron App** - Desktop app with web technologies
- **Progressive Web App** - Browser-based, installable
- **React/Vue Native** - Cross-platform mobile apps
- **Next.js/Nuxt** - Server-side rendered web interface

#### 🖥️ **Native Desktop**

- **C# + WPF/WinUI** - Native Windows integration
- **C++ + Qt** - High-performance cross-platform
- **Rust + Tauri** - Modern, secure desktop apps
- **Swift + SwiftUI** - Native macOS applications

#### 📱 **Mobile Platforms**

- **React Native** - iOS and Android from single codebase
- **Flutter** - Google's cross-platform framework
- **Native iOS** - Swift + UIKit/SwiftUI
- **Native Android** - Kotlin + Jetpack Compose

#### ☁️ **Web Services**

- **Node.js + Express** - Web API and dashboard
- **Go + Gin** - High-performance web service
- **Python + FastAPI** - API-first architecture
- **Rust + Axum** - Memory-safe web services

### 🎯 **What I Hope to See**

**For Users:**

- Drag-and-drop model management
- Voice interaction capabilities
- Multi-modal support (text, image, audio)
- Plugin/extension system
- Cloud sync and sharing

**For Developers:**

- Clean, documented APIs
- Modular architecture
- Comprehensive testing
- Performance optimizations
- Advanced security features

**For the Ecosystem:**

- Multiple language implementations
- Specialized use-case variants
- Integration with other AI tools
- Enterprise-ready versions

### 💡 **Getting Involved**

**Ways to Contribute:**

🔨 **Improve This Version**

- Add features, fix bugs, improve UI/UX
- Enhance security and performance
- Write documentation and tutorials

🚀 **Create New Implementations**

- Port to your favorite language/framework
- Optimize for specific platforms or use cases
- Add specialized features for different audiences

🌟 **Spread the Vision**

- Share with other developers
- Write blog posts about local AI UX
- Create videos and tutorials
- Build on the concept

### 📞 **Let's Collaborate**

If you're working on a port or major enhancement:

📧 **Email**: john@blancuzzi.org  
🐙 **GitHub**: [@jblancuzzi](https://github.com/jblancuzzi)  
💬 **Discussions**: Use GitHub Issues and Discussions

**I'd love to:**

- Feature your implementation in this README
- Collaborate on shared standards and APIs
- Cross-promote compatible projects
- Share lessons learned and best practices

---

## 🧪 Testing

ShamaOllama includes comprehensive tests to ensure quality and reliability.

### Running Tests

```bash
# Run all tests
python tests/run_all_tests.py

# Individual tests
python tests/test_thinking_filter.py
python tests/test_gpu_detection.py
python tests/test_dependencies.py
```

### Test Coverage

- **🧠 Thinking Models** - DeepSeek filtering patterns
- **🎮 GPU Detection** - Plugin architecture validation
- **📦 Dependencies** - Optional package management
- **🔒 Security** - Input validation and sanitization
- **⚙️ Core App** - Main functionality testing

For detailed test documentation, see [tests/README.md](tests/README.md).

---

## Contributing

This is a personal project that welcomes community involvement! The modular architecture makes it easy to extend with new features.

### Ways to Contribute

- 🐛 **Report bugs** and suggest improvements
- 💡 **Request features** that would improve usability
- 🔧 **Submit pull requests** with enhancements
- 📖 **Improve documentation** and tutorials
- 🚀 **Create ports** in other languages/frameworks
- ⭐ **Star the repository** to show support

For detailed contribution guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Copyright

Copyright (c) 2025 Jonathan Blancuzzi. All rights reserved.

## Author

**Jonathan Blancuzzi**

- Email: jblancuzzi@cupid.org
- Organization: Cupid Foundations, Inc.
- GitHub: [@jblancuzzi](https://github.com/jblancuzzi)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Guidelines

1. **Fork the repository**
2. **Create a feature branch** (`git checkout -b feature/AmazingFeature`)
3. **Commit your changes** (`git commit -m 'Add some AmazingFeature'`)
4. **Push to the branch** (`git push origin feature/AmazingFeature`)
5. **Open a Pull Request**

### Code Style

- Follow PEP 8 style guidelines
- Use type hints where appropriate
- Include docstrings for all functions and classes
- Keep functions focused and modular

## Acknowledgments

- **[Ollama](https://ollama.ai/)** - For providing the excellent local LLM platform
- **[CustomTkinter](https://github.com/TomSchimansky/CustomTkinter)** - For the modern GUI framework
- **[Python Community](https://www.python.org/)** - For the amazing ecosystem

## Roadmap

Future enhancements planned:

- **Real-time model parameters** (temperature, top_p, etc.)
- **Multi-model conversations** and comparison mode
- **Plugin system** for custom extensions
- **Advanced search** in chat history
- **Conversation templates** and prompt library
- **Model performance metrics** and benchmarking
- **Docker containerization** for easy deployment
- **Web interface** option alongside desktop GUI

## Support

If you encounter any issues or have questions:

1. **Check the [Issues](https://github.com/jblancuzzi/ollama-gui/issues)** page
2. **Review the [Troubleshooting](#troubleshooting)** section
3. **Create a new issue** with detailed information

## Star History

⭐ **Star this project** if you find it useful!

---

**Built with ❤️ using CustomTkinter for a modern, professional Ollama experience.**
